{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62bcbd9b",
   "metadata": {},
   "source": [
    "Generate separated files for each book containing the list of sentences. Sentence divisions is performed in a semi-manual way using specific patterns: only full stop is included when it is preceded by a token longer than 2 digits (i.e., p., M., but also ea). Inconsistencies and errors are present in the sentence split. The sentence count is at a book level: this means that sentence 23 in Book 4 is the 23rd sentence in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8916350",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/intermediate/xml_Clerice_to_csv.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ce0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_Clerice_to_csv = pd.read_csv(path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract the book from the reference and create a new column\n",
    "book_column = []\n",
    "pattern = r'(\\d+)(?:\\.(\\d))?\\.'\n",
    "\n",
    "for i, reference in enumerate(xml_Clerice_to_csv['reference']):\n",
    "    \n",
    "    match = re.search(pattern,reference)\n",
    "    book_column.append(int(match[1]))\n",
    "    \n",
    "xml_Clerice_to_csv['book'] = book_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73796533",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = ['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa785e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_column = [] ## create a new column for the sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,38): ## for each book of the NH\n",
    "        \n",
    "    # filter_book = LatLemm[LatLemm['book'] == i]\n",
    "    filter_book = xml_Clerice_to_csv[xml_Clerice_to_csv['book'] == i]\n",
    "    filter_book = filter_book.reset_index()\n",
    "    \n",
    "    count_sentence = 0\n",
    "    \n",
    "    for index,token in enumerate(filter_book['token']):\n",
    "        sentence_column.append(count_sentence)\n",
    "        \n",
    "        if token in punctuation:\n",
    "            \n",
    "            if len(filter_book['token'][index-1]) < 3: ## if the full stop is preceded by a token shorter than 3 digits\n",
    "                print(filter_book['index'][index-1], filter_book['token'][index-1]) ## do not increment the sentence count\n",
    "                \n",
    "            else : count_sentence = count_sentence + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905378fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_Clerice_to_csv['sentence'] = sentence_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f31db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_Clerice_to_csv.to_csv('data/intermediate/NH_lat_groupedsentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98da4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "## filter all the rows containing sentence = 0\n",
    "filter_book37 = xml_Clerice_to_csv[xml_Clerice_to_csv['book'] == 37]\n",
    "print_sent_0 = filter_book37[filter_book37['sentence'] == 0]\n",
    "\n",
    "## concatenate the tokens in filtered_rows\n",
    "concatenate_string = ' '.join(print_sent_0['token'].astype(str))\n",
    "concatenate_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd20c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write files containing the sentences\n",
    "for i in range(1,38): ## for each book of the NH\n",
    "        \n",
    "    filter_book = xml_Clerice_to_csv[xml_Clerice_to_csv['book'] == i]\n",
    "    name='data/intermediate/books/latbook'+str(i)\n",
    "    with open(name, \"w+\", encoding='utf-8') as file:\n",
    "        for i in range(0, filter_book['sentence'].max()+1):\n",
    "            filter_rows = filter_book[filter_book['sentence'] == i]\n",
    "            concatenate_sentence = ' '.join(filter_rows['token'].astype(str))\n",
    "            file.write(f\"{concatenate_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a31abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('NOW YOU NEED TO RUN Use_SimAlign_Tokens_and_Project_annotations.ipynb OR vecalign.py?? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b123008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes input generated in the Use_SimAlign_Tokens_and_Project_annotations.ipynb notebook. \n",
    "# path = \"data/intermediate/NH_lat_projected.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected = pd.read_csv(path, encoding='utf-8', dtype={'reference': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018caca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_in_sentence_column = []\n",
    "# index_in_sentence = 0\n",
    "\n",
    "# for i,token in enumerate(NH_projected['token']):\n",
    "#     print(i)\n",
    "#     if i == 0:\n",
    "#         index_in_sentence_column.append(index_in_sentence)\n",
    "#     if i > 0:\n",
    "#         if NH_projected['sentence'][i] == NH_projected['sentence'][i-1]: ## it is still in the same sentence\n",
    "#             index_in_sentence = index_in_sentence+1 ## increment\n",
    "#             index_in_sentence_column.append(index_in_sentence)\n",
    "#         else : \n",
    "#             index_in_sentence = 0 # reset the index\n",
    "#             index_in_sentence_column.append(index_in_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(index_in_sentence_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacba0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected['index_in_sentence'] = index_in_sentence_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067632cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NH_projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected.to_csv('NH_projected_index_sentences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8257a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected_sentences = NH_projected[['index_in_sentence']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NH_projected_sentences.to_csv('index_in_sentences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7552d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/Users/u0154817/OneDrive - KU Leuven/Documents/KU Leuven/PhD project 'Greek Spaces in Roman Times'/Data_Extraction/Outputs/table_book_(chapter)_chapter_paragraph.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2059b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_of_references = pd.read_csv(path, delimiter=';', encoding='utf-8', dtype={'reference': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65def974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_of_references.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cede098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_of_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bd211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# book_column = []\n",
    "# chapter_column = []\n",
    "# chapter_in_bracket_column = []\n",
    "\n",
    "# for book in range(1,38): ## for each book of the NH\n",
    "        \n",
    "#     filter_book = table_of_references[table_of_references['Book'] == book]\n",
    "#     filter_book = filter_book.reset_index()\n",
    "    \n",
    "#     unique_chapters = filter_book['Chapter'].unique().tolist() ## get a unique list of chapter numbers\n",
    "#     chapter_column.extend(unique_chapters)\n",
    "#     print(book, unique_chapters)\n",
    "    \n",
    "#     for chapter in unique_chapters:\n",
    "#         book_column.append(book)\n",
    "#         temp_chapter_in_brackets = []\n",
    "        \n",
    "#         for i,value in enumerate(filter_book['Chapter']):\n",
    "#             if value == chapter:\n",
    "#                 if filter_book['(Chapter)'][i] in temp_chapter_in_brackets:\n",
    "#                     continue\n",
    "#                 else : temp_chapter_in_brackets.append(filter_book['(Chapter)'][i])\n",
    "        \n",
    "#         chapter_in_bracket_column.append(temp_chapter_in_brackets)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022773b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reference = pd.DataFrame({'book': book_column, 'chapter_Perseus': chapter_column, '(chapters)_ToposText': chapter_in_bracket_column})\n",
    "# reference.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference.to_csv('chapter_correspondences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac9227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
